{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT \n",
    "## Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load in modules we will need \"\"\"\n",
    "import os\n",
    "import gc\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime as dt\n",
    "load_dotenv()\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Reading\" from Bronze Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieve all the csv file names from a given path\n",
    "\n",
    "Parameters: \n",
    "path (string)- Path we are looking through\n",
    "\n",
    "Returns: a list of .csv files\n",
    "\"\"\"\n",
    "def grab_files(path: str) -> list:\n",
    "    return [file for file in os.listdir(path) if file.endswith(\".csv\")]\n",
    "\n",
    "\"\"\"\n",
    "Creates DataFrames for all data of a specific entity in \"Bronze layer\"\n",
    "\n",
    "Parameters: \n",
    "path (string) - The \"container\" for the data \n",
    "\n",
    "Returns: a list of .csv files\n",
    "\"\"\"\n",
    "def gen_dfs(ospath = 'BRONZE_LAYER', path:str =''):    \n",
    "    dfs = []\n",
    "    try:\n",
    "        data_dir = f'{os.environ[ospath]}{path + \"/\" if path != \"\" else path}'\n",
    "        files = grab_files(data_dir)\n",
    "        for f in files:\n",
    "            df = pd.read_csv(data_dir+f)\n",
    "            df.name = f.split('__')[1]\n",
    "            dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error with pathing\")\n",
    "    except:\n",
    "        print('Error Occurred while extracting data from the file')\n",
    "    return dfs        #create the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM\n",
    "## Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Will Attempt to Drop all declared columns from a DataFrame\n",
    "\n",
    "Parameters: \n",
    "df - a singular Data Frame\n",
    "cols - a list of strings containing column attributes to be dropped\n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def drop_dfcols(df, cols:list):\n",
    "    try:\n",
    "        df.drop(columns = cols, inplace = True)\n",
    "    except KeyError:\n",
    "        print(f'fail to remove{cols}')\n",
    "\n",
    "\"\"\"\n",
    "Will Attempt to Drop all declared columns from a List of DataFrame\n",
    "\n",
    "Parameters: \n",
    "df - a List of Data Frame\n",
    "cols - a list of strings containing column attributes to be dropped \n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def drop_dfs_col(dfs, cols: list):\n",
    "    try:\n",
    "        for df in dfs:\n",
    "            drop_dfcols(df,cols) \n",
    "    except KeyError:\n",
    "        print('no columns left')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove duplicate records from the DataFrame\n",
    "\"\"\"\n",
    "def rm_dupe(df):\n",
    "    df.drop_duplicates(inplace= True)\n",
    "\n",
    "\"\"\"\n",
    "Drops the columns that have null vals\n",
    "or we fill them will default values if we want\n",
    "\"\"\"\n",
    "def rm_nulls(df, col, default_val = None):\n",
    "    if default_val:\n",
    "        df[col].fillna(default_val, inplace = True)\n",
    "    else:\n",
    "        df.dropna(subset =[col], inplace= True) \n",
    "\n",
    "\"\"\"\n",
    "Checks to see if any column of a DataFrame contains Nulls\n",
    "-best to do this after dropping useless columns [faster]\n",
    "\"\"\"\n",
    "def cols_with_null(df):\n",
    "    has_null = []\n",
    "    for col in df.columns:\n",
    "        print(f'{col} : {df[col].isnull().values.any()}')\n",
    "        if df[col].isnull().values.any():\n",
    "            has_null.append(col)\n",
    "    return has_null\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "dfs: List of DataFrames \n",
    "Return: Merged DataFrames \n",
    "\"\"\"\n",
    "def merge_dfs(dfs: list):\n",
    "    name_attr = dfs[0].name if hasattr(dfs[0], \"name\") else None\n",
    "    dfs = pd.concat(dfs, ignore_index=True)        #put all the data into one table\n",
    "    if name_attr:\n",
    "        dfs.name = name_attr\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checks to see if a val is of type : etype\n",
    "\n",
    "Parameters:\n",
    "etype: Type \n",
    "\n",
    "Returns:\n",
    "Boolean if it is that type\n",
    "\"\"\"\n",
    "def is_type(val,  etype):\n",
    "    try:\n",
    "        etype(val)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "    \n",
    "\"\"\"\n",
    "Removes all the records that have an invalid type\n",
    "\n",
    "Parameters:\n",
    "header_df : DataFrame\n",
    "col : column name to check\n",
    "etype: Type \n",
    "\n",
    "Returns:\n",
    "DataFrame with records that have valid type\n",
    "\"\"\"\n",
    "def rm_inval_type(df, col, etype):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[df[col].apply(lambda val: is_type(val,etype))]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Removes outliers (using Interquartile range) from a given column\n",
    "Parameters:\n",
    "df: DataFrame\n",
    "col: column to check for outliers\n",
    "Returns: DataFrame with outliers removed\n",
    "\"\"\"\n",
    "def IQR_outlier(df, col: str):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    Q1 = df[col].quantile(0.25)   \n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    # non_outliers = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    df_rm_outliers = df[~((df[col] < lower_bound) | (df[col] > upper_bound))]\n",
    "    if name_attr:\n",
    "        df_rm_outliers.name = name_attr\n",
    "    return df_rm_outliers\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Removes outliers (using Z-score) from a given column\n",
    "Parameters:\n",
    "df: DataFrame\n",
    "col: column to check for outliers\n",
    "deviance: outlier bounds\n",
    "Returns: DataFrame with outliers removed\n",
    "\"\"\"\n",
    "def zscore(df, col:str, deviance:int):\n",
    "    col_mean = df[col].mean()\n",
    "    col_std = df[col].std()\n",
    "    return df[abs((df[col] - col_mean)/ col_std) <= deviance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD\n",
    "### Save into Currated layer [Silver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Will file \"modded_{filename}.csv\"\n",
    "in curated-layer directory.\n",
    "If path does not exist, will create it for you.\n",
    "\n",
    "Parameters: \n",
    "df - a Data Frame\n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def df2csv(df):\n",
    "    year = df.name[-4:]\n",
    "    type = df.name[:-5]\n",
    "    path = f'../data_set/CBP_AMS/curated-layer/{year}/{type}/'          \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    df.to_csv(path + f'modded_{df.name}.csv', sep='|' , index=False) \n",
    "\n",
    "    \"\"\"\n",
    "Will file \"modded_{filename}.parquet\"\n",
    "in curated-layer directory.\n",
    "If path does not exist, will create it for you.\n",
    "\n",
    "Parameters: \n",
    "df - a Data Frame\n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def df2parquet(df):\n",
    "    year = df.name[-4:]\n",
    "    type = df.name[:-5]\n",
    "    path = f'../data_set/CBP_AMS/curated-layer/{year}/{type}/'          \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    df.to_parquet(path + f'modded_{df.name}.parquet', engine='pyarrow',index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Current Use Case : We can reduce data overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_billgen = ['house_bol_number','sub_house_bol_number', 'bill_type_code',\n",
    "              'manifest_number', 'trade_update_date', 'run_date']\n",
    "\n",
    "rm_container = ['seal_number_1', 'seal_number_2',\n",
    "       'equipment_description_code','container_type', 'load_status']\n",
    "\n",
    "rm_header  = ['carrier_code', 'vessel_country_code', 'vessel_name',\n",
    "       'foreign_port_of_lading_qualifier', 'manifest_quantity', 'manifest_unit', \n",
    "       'measurement', 'measurement_unit', 'record_status_indicator',\n",
    "       'place_of_receipt', 'port_of_destination', 'foreign_port_of_destination_qualifier', \n",
    "       'foreign_port_of_destination', 'conveyance_id_qualifier', 'conveyance_id', \n",
    "       'in_bond_entry_type', 'mode_of_transportation', 'secondary_notify_party_1',\n",
    "       'secondary_notify_party_2', 'secondary_notify_party_3', 'secondary_notify_party_4', \n",
    "       'secondary_notify_party_5', 'secondary_notify_party_6', 'secondary_notify_party_7',\n",
    "       'secondary_notify_party_8', 'secondary_notify_party_9', 'secondary_notify_party_10']\n",
    "\n",
    "rm_tariff = ['description_sequence_number','harmonized_number', 'harmonized_weight_unit','harmonized_weight']\n",
    "\n",
    "rm_cargodesc = [ 'description_sequence_number','description_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------HEADER--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Header Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert the weight column to a universal weight unit measurement\n",
    "\n",
    "Parameters:\n",
    "header_df: DataFrame\n",
    "\n",
    "Returns:\n",
    "None\n",
    "\"\"\"\n",
    "def universal_unit(header_df):\n",
    "    unit_weight = {\n",
    "        'Kilograms': 1.0,\n",
    "        'Pounds': 0.453592, \n",
    "        'Metric Ton': 1000.0, \n",
    "        'Long Ton': 1016.04691, \n",
    "        'Measurement Ton': 1.01604691\n",
    "    }\n",
    "    # Convert weight to kg based on weight_unit\n",
    "    for unit, scale in unit_weight.items():\n",
    "        header_df.loc[header_df['weight_unit'] == unit, 'weight'] *= scale\n",
    "        header_df['weight_unit'] = 'Kilograms'\n",
    "\n",
    "\n",
    "def is_date(val):\n",
    "    try:\n",
    "        dt.strptime(val, '%Y-%m-%d')\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def rm_inval_date(df,col):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[df[col].apply(is_date)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "removes the records that have dates that are not in the scope of this year\n",
    "\"\"\"\n",
    "def date_outlier(df):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    year = int(df.name[-4:])\n",
    "    df['estimated_arrival_date'] = pd.to_datetime(df['estimated_arrival_date'], format='%Y-%m-%d')\n",
    "    df['actual_arrival_date'] = pd.to_datetime(df['actual_arrival_date'], format='%Y-%m-%d')\n",
    "    df = df[df['actual_arrival_date'].dt.year <= year]   # we are not in the future!!!\n",
    "    df = df[(abs(df['estimated_arrival_date'].dt.year - year) <= 5)]\n",
    "    df = df[(abs(df['actual_arrival_date'].dt.year - year) <= 5)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in HEADER Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   \n",
    "dfs = gen_dfs(path='header')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(dfs, rm_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one \n",
    "dfs = merge_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records\n",
    "rm_dupe(dfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of any irregular data in the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure there are no null values 😀\n",
    "for col in cols_with_null(dfs):\n",
    "    rm_nulls(dfs, col)\n",
    "# Header does not need to deal with any Nulls 🎉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of weight columns \n",
    "dfs = rm_inval_type(dfs, 'weight', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of port\n",
    "dfs = rm_inval_type(dfs, 'port_of_unlading', str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of port\n",
    "dfs = rm_inval_type(dfs, 'foreign_port_of_lading', str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of unit\n",
    "dfs = rm_inval_type(dfs, 'weight_unit', str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid date in this col\n",
    "dfs = rm_inval_date(dfs, 'estimated_arrival_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid date in this col\n",
    "dfs = rm_inval_date(dfs, 'actual_arrival_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove out the Outliers\n",
    "dfs = IQR_outlier(dfs, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any date outliers \n",
    "dfs = date_outlier(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the weight unit to a singular unit ( Kilogram )\n",
    "universal_unit(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned up Header Data -> Store into \"Silver Layer\" \n",
    "df2parquet(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------BILL GEN--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   \n",
    "dfs = gen_dfs(path='billgen')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(dfs, rm_billgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one  \n",
    "dfs = merge_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records  \n",
    "rm_dupe(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any null values\n",
    "for col in cols_with_null(dfs):\n",
    "    rm_nulls(dfs, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned up Bill Gen Data -> Store into \"Silver Layer\" \n",
    "df2parquet(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------CONTAINER--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_box_outlier(df):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[(df['container_length'] != 0) & (df['container_width'] != 0) & (df['container_height'] != 0)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   \n",
    "dfs = gen_dfs(path='container')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(dfs, rm_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one    30635677 rows × 6 columns   27234627 rows × 6 columns\n",
    "dfs = merge_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records  \n",
    "rm_dupe(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any null values\n",
    "for col in cols_with_null(dfs):\n",
    "    rm_nulls(dfs, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid length in this col\n",
    "dfs = rm_inval_type(dfs, 'container_length', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid height in this col\n",
    "dfs = rm_inval_type(dfs, 'container_height', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid width in this col\n",
    "dfs = rm_inval_type(dfs, 'container_width', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = rm_box_outlier(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_counts = dfs.groupby(['container_length', 'container_height', 'container_width']).size().reset_index(name='counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_counts.sort_values('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2parquet(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------Tariff + Cargo Desc------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Tariff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_cgd_outliers(df):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[(df['piece_count'] != 0)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df\n",
    "\n",
    "def rm_tariff_outliers(df):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[(df['harmonized_value'] > 0)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   |   cargodesc\n",
    "tdfs = gen_dfs(path='tariff')\n",
    "tdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(tdfs, rm_tariff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one   \n",
    "tdfs = merge_dfs(tdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records  \n",
    "rm_dupe(tdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any null values\n",
    "for col in cols_with_null(tdfs):\n",
    "    rm_nulls(tdfs, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of piece\n",
    "tdfs = rm_inval_type(tdfs,'harmonized_value', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove value outliers\n",
    "tdfs = rm_tariff_outliers(tdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdfs = IQR_outlier(tdfs, 'harmonized_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Cargo Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   |   cargodesc\n",
    "dfs = gen_dfs(path='cargodesc')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(dfs, rm_cargodesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one    30635677 rows × 6 columns   27234627 rows × 6 columns\n",
    "dfs = merge_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records  \n",
    "rm_dupe(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any null values\n",
    "for col in cols_with_null(dfs):\n",
    "    rm_nulls(dfs, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of piece\n",
    "dfs = rm_inval_type(dfs,'piece_count', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = rm_cgd_outliers(dfs)\n",
    "# done cleaning cargo desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdfs = pd.merge(dfs,tdfs, on=['identifier', 'container_number'])\n",
    "mdfs.name = tdfs.name[:-5] + '_' + dfs.name[:-5] + '_' + tdfs.name[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdfs['tot_cost'] = mdfs['piece_count'] * mdfs['harmonized_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdfs.sort_values('container_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2parquet(mdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Data Into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "def grab_files(path: str) -> list:\n",
    "    file=[file for file in os.listdir(path) if file.endswith(\".parquet\")]\n",
    "    return file[0]\n",
    "\n",
    "def load_pq_df(model):\n",
    "    path = f'{os.environ['SILVER_LAYER']}{model}/'\n",
    "    file = grab_files(path)\n",
    "    df = pd.read_parquet(path + file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pyodbc.connect('DRIVER={SQL Server};Server={DESKTOP-N9SA336};Database=BastionSLA')\n",
    "cursor = conn.cursor() # Cursors are database level objects that let your query a database multiple times - Think of it as a pointer to a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyodbc.Cursor at 0x1b5ae138430>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DB as Silver Layer Storage\n",
    "db_name = \"BastionSLA\"\n",
    "cursor.execute(f\"\"\"\n",
    "        IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '{db_name}')\n",
    "        BEGIN\n",
    "            CREATE DATABASE {db_name}\n",
    "        END\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyodbc.Cursor at 0x1b5a5ba3630>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('use BastionSLA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pq_df('billgen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'billgen'\n",
    "create_table_query = f\"\"\"\n",
    "                IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{table_name}')\n",
    "                BEGIN\n",
    "                    CREATE TABLE {table_name} (\n",
    "                        identifier INT,\n",
    "                        master_bol_number VARCHAR(255),\n",
    "                        voyage_number VARCHAR(50)\n",
    "                    )\n",
    "                END\n",
    "                \"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvin\\AppData\\Local\\Temp\\ipykernel_28532\\1548497140.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df.to_sql('billgen', conn, index=False)\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\n        SELECT\n            name\n        FROM\n            sqlite_master\n        WHERE\n            type IN ('table', 'view')\n            AND name=?;\n        ': ('42S02', \"[42S02] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid object name 'sqlite_master'. (208) (SQLExecDirectW); [42S02] [Microsoft][ODBC SQL Server Driver][SQL Server]Statement(s) could not be prepared. (8180)\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\alvin\\OneDrive\\Desktop\\Codelioco\\SkillStorm\\.venv\\Lib\\site-packages\\pandas\\io\\sql.py:2264\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2263\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2264\u001b[0m     cur\u001b[39m.\u001b[39;49mexecute(sql, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m   2265\u001b[0m     \u001b[39mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mProgrammingError\u001b[0m: ('42S02', \"[42S02] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid object name 'sqlite_master'. (208) (SQLExecDirectW); [42S02] [Microsoft][ODBC SQL Server Driver][SQL Server]Statement(s) could not be prepared. (8180)\")",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alvin\\OneDrive\\Desktop\\Codelioco\\SkillStorm\\Projects\\Project_1\\pipeline\\ETL.ipynb Cell 89\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alvin/OneDrive/Desktop/Codelioco/SkillStorm/Projects/Project_1/pipeline/ETL.ipynb#Y215sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mto_sql(\u001b[39m'\u001b[39;49m\u001b[39mbillgen\u001b[39;49m\u001b[39m'\u001b[39;49m, conn, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\alvin\\OneDrive\\Desktop\\Codelioco\\SkillStorm\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alvin\\OneDrive\\Desktop\\Codelioco\\SkillStorm\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3008\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[1;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   2813\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2814\u001b[0m \u001b[39mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[0;32m   2815\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3004\u001b[0m \u001b[39m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[0;32m   3005\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[0;32m   3006\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m sql\n\u001b[1;32m-> 3008\u001b[0m \u001b[39mreturn\u001b[39;00m sql\u001b[39m.\u001b[39;49mto_sql(\n\u001b[0;32m   3009\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   3010\u001b[0m     name,\n\u001b[0;32m   3011\u001b[0m     con,\n\u001b[0;32m   3012\u001b[0m     schema\u001b[39m=\u001b[39;49mschema,\n\u001b[0;32m   3013\u001b[0m     if_exists\u001b[39m=\u001b[39;49mif_exists,\n\u001b[0;32m   3014\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   3015\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3016\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3017\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   3018\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m   3019\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\alvin\\OneDrive\\Desktop\\Codelioco\\SkillStorm\\.venv\\Lib\\site-packages\\pandas\\io\\sql.py:788\u001b[0m, in \u001b[0;36mto_sql\u001b[1;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    784\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m'\u001b[39m\u001b[39m argument should be either a Series or a DataFrame\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    785\u001b[0m     )\n\u001b[0;32m    787\u001b[0m \u001b[39mwith\u001b[39;00m pandasSQL_builder(con, schema\u001b[39m=\u001b[39mschema, need_transaction\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 788\u001b[0m     \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mto_sql(\n\u001b[0;32m    789\u001b[0m         frame,\n\u001b[0;32m    790\u001b[0m         name,\n\u001b[0;32m    791\u001b[0m         if_exists\u001b[39m=\u001b[39;49mif_exists,\n\u001b[0;32m    792\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m    793\u001b[0m         index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m    794\u001b[0m         schema\u001b[39m=\u001b[39;49mschema,\n\u001b[0;32m    795\u001b[0m         chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m    796\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    797\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m    798\u001b[0m         engine\u001b[39m=\u001b[39;49mengine,\n\u001b[0;32m    799\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mengine_kwargs,\n\u001b[0;32m    800\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alvin\\OneDrive\\Desktop\\Codelioco\\SkillStorm\\.venv\\Lib\\site-packages\\pandas\\io\\sql.py:2440\u001b[0m, in \u001b[0;36mSQLiteDatabase.to_sql\u001b[1;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m   2429\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcol\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mmy_type\u001b[39m}\u001b[39;00m\u001b[39m) not a string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2431\u001b[0m table \u001b[39m=\u001b[39m SQLiteTable(\n\u001b[0;32m   2432\u001b[0m     name,\n\u001b[0;32m   2433\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2438\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   2439\u001b[0m )\n\u001b[1;32m-> 2440\u001b[0m table\u001b[39m.\u001b[39;49mcreate()\n\u001b[0;32m   2441\u001b[0m \u001b[39mreturn\u001b[39;00m table\u001b[39m.\u001b[39minsert(chunksize, method)\n",
      "File \u001b[1;32mc:\\Users\\alvin\\OneDrive\\Desktop\\Codelioco\\SkillStorm\\.venv\\Lib\\site-packages\\pandas\\io\\sql.py:925\u001b[0m, in \u001b[0;36mSQLTable.create\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 925\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexists():\n\u001b[0;32m    926\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mif_exists \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfail\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    927\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTable \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m already exists.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alvin\\OneDrive\\Desktop\\Codelioco\\SkillStorm\\.venv\\Lib\\site-packages\\pandas\\io\\sql.py:911\u001b[0m, in \u001b[0;36mSQLTable.exists\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexists\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 911\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpd_sql\u001b[39m.\u001b[39;49mhas_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mschema)\n",
      "File \u001b[1;32mc:\\Users\\alvin\\OneDrive\\Desktop\\Codelioco\\SkillStorm\\.venv\\Lib\\site-packages\\pandas\\io\\sql.py:2455\u001b[0m, in \u001b[0;36mSQLiteDatabase.has_table\u001b[1;34m(self, name, schema)\u001b[0m\n\u001b[0;32m   2444\u001b[0m wld \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2445\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m   2446\u001b[0m \u001b[39mSELECT\u001b[39m\n\u001b[0;32m   2447\u001b[0m \u001b[39m    name\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2452\u001b[0m \u001b[39m    AND name=\u001b[39m\u001b[39m{\u001b[39;00mwld\u001b[39m}\u001b[39;00m\u001b[39m;\u001b[39m\n\u001b[0;32m   2453\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m-> 2455\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(query, [name])\u001b[39m.\u001b[39mfetchall()) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alvin\\OneDrive\\Desktop\\Codelioco\\SkillStorm\\.venv\\Lib\\site-packages\\pandas\\io\\sql.py:2276\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2273\u001b[0m     \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39minner_exc\u001b[39;00m\n\u001b[0;32m   2275\u001b[0m ex \u001b[39m=\u001b[39m DatabaseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecution failed on sql \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msql\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2276\u001b[0m \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\n        SELECT\n            name\n        FROM\n            sqlite_master\n        WHERE\n            type IN ('table', 'view')\n            AND name=?;\n        ': ('42S02', \"[42S02] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid object name 'sqlite_master'. (208) (SQLExecDirectW); [42S02] [Microsoft][ODBC SQL Server Driver][SQL Server]Statement(s) could not be prepared. (8180)\")"
     ]
    }
   ],
   "source": [
    "df.to_sql('billgen', conn, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
