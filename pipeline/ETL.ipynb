{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT \n",
    "## Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load in modules we will need \"\"\"\n",
    "import os\n",
    "import gc\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "load_dotenv()\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Reading\" from Bronze Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieve all the csv file names from a given path\n",
    "\n",
    "Parameters: \n",
    "path (string)- Path we are looking through\n",
    "\n",
    "Returns: a list of .csv files\n",
    "\"\"\"\n",
    "def grab_files(path: str) -> list:\n",
    "    return [file for file in os.listdir(path) if file.endswith(\".csv\")]\n",
    "\n",
    "\"\"\"\n",
    "Creates DataFrames for all data of a specific entity in \"Bronze layer\"\n",
    "\n",
    "Parameters: \n",
    "path (string) - The \"container\" for the data \n",
    "\n",
    "Returns: a list of .csv files\n",
    "\"\"\"\n",
    "def gen_dfs(ospath = 'BRONZE_LAYER', path:str =''):    \n",
    "    dfs = []\n",
    "    try:\n",
    "        data_dir = f'{os.environ[ospath]}{path + \"/\" if path != \"\" else path}'\n",
    "        files = grab_files(data_dir)\n",
    "        for f in files:\n",
    "            df = pd.read_csv(data_dir+f)\n",
    "            df.name = f.split('__')[1]\n",
    "            dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error with pathing\")\n",
    "    except:\n",
    "        print('Error Occurred while extracting data from the file')\n",
    "    return dfs        #create the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM\n",
    "## Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Will Attempt to Drop all declared columns from a DataFrame\n",
    "\n",
    "Parameters: \n",
    "df - a singular Data Frame\n",
    "cols - a list of strings containing column attributes to be dropped\n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def drop_dfcols(df, cols:list):\n",
    "    try:\n",
    "        df.drop(columns = cols, inplace = True)\n",
    "    except KeyError:\n",
    "        print(f'fail to remove{cols}')\n",
    "\n",
    "\"\"\"\n",
    "Will Attempt to Drop all declared columns from a List of DataFrame\n",
    "\n",
    "Parameters: \n",
    "df - a List of Data Frame\n",
    "cols - a list of strings containing column attributes to be dropped \n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def drop_dfs_col(dfs, cols: list):\n",
    "    try:\n",
    "        for df in dfs:\n",
    "            drop_dfcols(df,cols) \n",
    "    except KeyError:\n",
    "        print('no columns left')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove duplicate records from the DataFrame\n",
    "\"\"\"\n",
    "def rm_dupe(df):\n",
    "    df.drop_duplicates(inplace= True)\n",
    "\n",
    "\"\"\"\n",
    "Drops the columns that have null vals\n",
    "or we fill them will default values if we want\n",
    "\"\"\"\n",
    "def rm_nulls(df, col, default_val = None):\n",
    "    if default_val:\n",
    "        df[col].fillna(default_val, inplace = True)\n",
    "    else:\n",
    "        df.dropna(subset =[col], inplace= True) \n",
    "\n",
    "\"\"\"\n",
    "Checks to see if any column of a DataFrame contains Nulls\n",
    "-best to do this after dropping useless columns [faster]\n",
    "\"\"\"\n",
    "def cols_with_null(df):\n",
    "    has_null = []\n",
    "    for col in df.columns:\n",
    "        print(f'{col} : {df[col].isnull().values.any()}')\n",
    "        if df[col].isnull().values.any():\n",
    "            has_null.append(col)\n",
    "    return has_null\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "dfs: List of DataFrames \n",
    "Return: Merged DataFrames \n",
    "\"\"\"\n",
    "def merge_dfs(dfs: list):\n",
    "    name_attr = dfs[0].name if hasattr(dfs[0], \"name\") else None\n",
    "    dfs = pd.concat(dfs, ignore_index=True)        #put all the data into one table\n",
    "    if name_attr:\n",
    "        dfs.name = name_attr\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checks to see if a val is of type : etype\n",
    "\n",
    "Parameters:\n",
    "etype: Type \n",
    "\n",
    "Returns:\n",
    "Boolean if it is that type\n",
    "\"\"\"\n",
    "def is_type(val,  etype):\n",
    "    try:\n",
    "        etype(val)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "    \n",
    "\"\"\"\n",
    "Removes all the records that have an invalid type\n",
    "\n",
    "Parameters:\n",
    "header_df : DataFrame\n",
    "col : column name to check\n",
    "etype: Type \n",
    "\n",
    "Returns:\n",
    "DataFrame with records that have valid type\n",
    "\"\"\"\n",
    "def rm_inval_type(df, col, etype):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[df[col].apply(lambda val: is_type(val,etype))]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Removes outliers (using Interquartile range) from a given column\n",
    "Parameters:\n",
    "df: DataFrame\n",
    "col: column to check for outliers\n",
    "Returns: DataFrame with outliers removed\n",
    "\"\"\"\n",
    "def IQR_outlier(df, col: str):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    Q1 = df[col].quantile(0.25)   \n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    # non_outliers = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    df_rm_outliers = df[~((df[col] < lower_bound) | (df[col] > upper_bound))]\n",
    "    if name_attr:\n",
    "        df_rm_outliers.name = name_attr\n",
    "    return df_rm_outliers\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Removes outliers (using Z-score) from a given column\n",
    "Parameters:\n",
    "df: DataFrame\n",
    "col: column to check for outliers\n",
    "deviance: outlier bounds\n",
    "Returns: DataFrame with outliers removed\n",
    "\"\"\"\n",
    "def zscore(df, col:str, deviance:int):\n",
    "    col_mean = df[col].mean()\n",
    "    col_std = df[col].std()\n",
    "    return df[abs((df[col] - col_mean)/ col_std) <= deviance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD\n",
    "### Save into Currated layer [Silver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Will file \"modded_{filename}.csv\"\n",
    "in curated-layer directory.\n",
    "If path does not exist, will create it for you.\n",
    "\n",
    "Parameters: \n",
    "df - a Data Frame\n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def df2csv(df):\n",
    "    year = df.name[-4:]\n",
    "    type = df.name[:-5]\n",
    "    path = f'../data_set/CBP_AMS/curated-layer/{year}/{type}/'          \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    df.to_csv(path + f'modded_{df.name}.csv', sep='|' , index=False) \n",
    "\n",
    "    \"\"\"\n",
    "Will file \"modded_{filename}.parquet\"\n",
    "in curated-layer directory.\n",
    "If path does not exist, will create it for you.\n",
    "\n",
    "Parameters: \n",
    "df - a Data Frame\n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def df2parquet(df):\n",
    "    year = df.name[-4:]\n",
    "    type = df.name[:-5]\n",
    "    path = f'../data_set/CBP_AMS/curated-layer/{year}/{type}/'          \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    df.to_parquet(path + f'modded_{df.name}.parquet', engine='pyarrow',index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Current Use Case : We can reduce data overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_billgen = ['house_bol_number','sub_house_bol_number', 'bill_type_code',\n",
    "              'manifest_number', 'trade_update_date', 'run_date']\n",
    "\n",
    "rm_container = ['seal_number_1', 'seal_number_2',\n",
    "       'equipment_description_code','container_type', 'load_status']\n",
    "\n",
    "rm_header  = ['carrier_code', 'vessel_country_code', 'vessel_name',\n",
    "       'foreign_port_of_lading_qualifier', 'manifest_quantity', 'manifest_unit', \n",
    "       'measurement', 'measurement_unit', 'record_status_indicator',\n",
    "       'place_of_receipt', 'port_of_destination', 'foreign_port_of_destination_qualifier', \n",
    "       'foreign_port_of_destination', 'conveyance_id_qualifier', 'conveyance_id', \n",
    "       'in_bond_entry_type', 'mode_of_transportation', 'secondary_notify_party_1',\n",
    "       'secondary_notify_party_2', 'secondary_notify_party_3', 'secondary_notify_party_4', \n",
    "       'secondary_notify_party_5', 'secondary_notify_party_6', 'secondary_notify_party_7',\n",
    "       'secondary_notify_party_8', 'secondary_notify_party_9', 'secondary_notify_party_10']\n",
    "\n",
    "rm_tariff = ['description_sequence_number','harmonized_number', 'harmonized_weight_unit','harmonized_weight']\n",
    "\n",
    "rm_cargodesc = [ 'description_sequence_number','description_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------HEADER--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Header Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert the weight column to a universal weight unit measurement\n",
    "\n",
    "Parameters:\n",
    "header_df: DataFrame\n",
    "\n",
    "Returns:\n",
    "None\n",
    "\"\"\"\n",
    "def universal_unit(header_df):\n",
    "    unit_weight = {\n",
    "        'Kilograms': 1.0,\n",
    "        'Pounds': 0.453592, \n",
    "        'Metric Ton': 1000.0, \n",
    "        'Long Ton': 1016.04691, \n",
    "        'Measurement Ton': 1.01604691\n",
    "    }\n",
    "    # Convert weight to kg based on weight_unit\n",
    "    for unit, scale in unit_weight.items():\n",
    "        header_df.loc[header_df['weight_unit'] == unit, 'weight'] *= scale\n",
    "        header_df['weight_unit'] = 'Kilograms'\n",
    "\n",
    "\n",
    "def is_date(val):\n",
    "    try:\n",
    "        dt.strptime(val, '%Y-%m-%d')\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def rm_inval_date(df,col):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[df[col].apply(is_date)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "removes the records that have dates that are not in the scope of this year\n",
    "\"\"\"\n",
    "def date_outlier(df):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    year = int(df.name[-4:])\n",
    "    df['estimated_arrival_date'] = pd.to_datetime(df['estimated_arrival_date'], format='%Y-%m-%d')\n",
    "    df['actual_arrival_date'] = pd.to_datetime(df['actual_arrival_date'], format='%Y-%m-%d')\n",
    "    df = df[df['actual_arrival_date'].dt.year <= year]   # we are not in the future!!!\n",
    "    df = df[(abs(df['estimated_arrival_date'].dt.year - year) <= 5)]\n",
    "    df = df[(abs(df['actual_arrival_date'].dt.year - year) <= 5)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in HEADER Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   \n",
    "dfs = gen_dfs(path='header')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(dfs, rm_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one \n",
    "dfs = merge_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records\n",
    "rm_dupe(dfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of any irregular data in the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure there are no null values ðŸ˜€\n",
    "for col in cols_with_null(dfs):\n",
    "    rm_nulls(dfs, col)\n",
    "# Header does not need to deal with any Nulls ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of weight columns \n",
    "dfs = rm_inval_type(dfs, 'weight', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of port\n",
    "dfs = rm_inval_type(dfs, 'port_of_unlading', str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of port\n",
    "dfs = rm_inval_type(dfs, 'foreign_port_of_lading', str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of unit\n",
    "dfs = rm_inval_type(dfs, 'weight_unit', str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid date in this col\n",
    "dfs = rm_inval_date(dfs, 'estimated_arrival_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid date in this col\n",
    "dfs = rm_inval_date(dfs, 'actual_arrival_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove out the Outliers\n",
    "dfs = IQR_outlier(dfs, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any date outliers \n",
    "dfs = date_outlier(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the weight unit to a singular unit ( Kilogram )\n",
    "universal_unit(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned up Header Data -> Store into \"Silver Layer\" \n",
    "df2parquet(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------BILL GEN--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   \n",
    "dfs = gen_dfs(path='billgen')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(dfs, rm_billgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one  \n",
    "dfs = merge_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records  \n",
    "rm_dupe(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any null values\n",
    "for col in cols_with_null(dfs):\n",
    "    rm_nulls(dfs, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned up Bill Gen Data -> Store into \"Silver Layer\" \n",
    "df2parquet(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------CONTAINER--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_box_outlier(df):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[(df['container_length'] != 0) & (df['container_width'] != 0) & (df['container_height'] != 0)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   \n",
    "dfs = gen_dfs(path='container')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(dfs, rm_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one    30635677 rows Ã— 6 columns   27234627 rows Ã— 6 columns\n",
    "dfs = merge_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records  \n",
    "rm_dupe(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any null values\n",
    "for col in cols_with_null(dfs):\n",
    "    rm_nulls(dfs, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid length in this col\n",
    "dfs = rm_inval_type(dfs, 'container_length', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid height in this col\n",
    "dfs = rm_inval_type(dfs, 'container_height', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid width in this col\n",
    "dfs = rm_inval_type(dfs, 'container_width', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = rm_box_outlier(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_counts = dfs.groupby(['container_length', 'container_height', 'container_width']).size().reset_index(name='counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_counts.sort_values('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2parquet(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------Tariff + Cargo Desc------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Tariff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_cgd_outliers(df):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[(df['piece_count'] != 0)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df\n",
    "\n",
    "def rm_tariff_outliers(df):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[(df['harmonized_value'] > 0)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   |   cargodesc\n",
    "tdfs = gen_dfs(path='tariff')\n",
    "tdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(tdfs, rm_tariff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one   \n",
    "tdfs = merge_dfs(tdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records  \n",
    "rm_dupe(tdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any null values\n",
    "for col in cols_with_null(tdfs):\n",
    "    rm_nulls(tdfs, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of piece\n",
    "tdfs = rm_inval_type(tdfs,'harmonized_value', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove value outliers\n",
    "tdfs = rm_tariff_outliers(tdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdfs = IQR_outlier(tdfs, 'harmonized_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Cargo Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   |   cargodesc\n",
    "dfs = gen_dfs(path='cargodesc')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(dfs, rm_cargodesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one    30635677 rows Ã— 6 columns   27234627 rows Ã— 6 columns\n",
    "dfs = merge_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records  \n",
    "rm_dupe(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any null values\n",
    "for col in cols_with_null(dfs):\n",
    "    rm_nulls(dfs, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of piece\n",
    "dfs = rm_inval_type(dfs,'piece_count', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = rm_cgd_outliers(dfs)\n",
    "# done cleaning cargo desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdfs = pd.merge(dfs,tdfs, on=['identifier', 'container_number'])\n",
    "mdfs.name = tdfs.name[:-5] + '_' + dfs.name[:-5] + '_' + tdfs.name[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdfs['tot_cost'] = mdfs['piece_count'] * mdfs['harmonized_value']       # not necessary for silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2parquet(mdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Data Into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "def grab_files(path: str) -> list:\n",
    "    file=[file for file in os.listdir(path) if file.endswith(\".parquet\")]\n",
    "    return file[0]\n",
    "\n",
    "def load_pq_df(model):\n",
    "    path = f'{os.environ['SILVER_LAYER']}{model}/'\n",
    "    file = grab_files(path)\n",
    "    df = pd.read_parquet(path + file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = urllib.parse.quote_plus(r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                                 r'SERVER=DESKTOP-N9SA336;'\n",
    "                                 r'DATABASE=BastionSLA;'\n",
    "                                 r'Trusted_Connection=yes;')\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};Server={DESKTOP-N9SA336};Database=;Trusted_Connection=yes;')\n",
    "cursor = conn.cursor() # Cursors are database level objects that let your query a database multiple times - Think of it as a pointer to a row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Bastion Analytic DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DB as Silver Layer Storage\n",
    "db_name = \"BastionSLA\"\n",
    "cursor.execute(f\"\"\"\n",
    "        IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '{db_name}')\n",
    "        BEGIN\n",
    "            CREATE DATABASE {db_name}\n",
    "        END\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('use BastionSLA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Bill Gen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pq_df('billgen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_billgen = 'billgen'\n",
    "try:\n",
    "    create_table_query = f\"\"\"\n",
    "                    IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{table_billgen}')\n",
    "                    BEGIN\n",
    "                        CREATE TABLE {table_billgen} (\n",
    "                            identifier BIGINT NOT NULL,\n",
    "                            master_bol_number VARCHAR(255),\n",
    "                            voyage_number VARCHAR(50)\n",
    "                        )\n",
    "                    END\n",
    "                    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print('Table has been created')\n",
    "except:\n",
    "    print('Alrdy exist or something went wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE BILLGEN to DATABASE\n",
    "df.to_sql(table_billgen, engine, if_exists='append', index=False, schema='dbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------ Load Container -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pq_df('container')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_container = 'container'\n",
    "try:\n",
    "    create_table_query = f\"\"\"\n",
    "                    IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{table_container}')\n",
    "                    BEGIN\n",
    "                        CREATE TABLE {table_container} (\n",
    "                            identifier BIGINT, \n",
    "                            container_number VARCHAR(50), \n",
    "                            container_length INT,\n",
    "                            container_height INT,\n",
    "                            container_width INT,\n",
    "                            type_of_service VARCHAR(255) \n",
    "                        )\n",
    "                    END\n",
    "                    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print('Table has been created')\n",
    "except:\n",
    "    print('Alrdy exist or something went wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE BILLGEN to DATABASE\n",
    "df.to_sql(table_container, engine, if_exists='append', index=False, schema='dbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------ Load Header -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pq_df('header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_header = 'header'\n",
    "try:\n",
    "    create_table_query = f\"\"\"\n",
    "                    IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{table_header}')\n",
    "                    BEGIN\n",
    "                        CREATE TABLE {table_header} (\n",
    "                            identifier BIGINT,\n",
    "                            port_of_unlading VARCHAR(100),\n",
    "                            estimated_arrival_date DATE,\n",
    "                            foreign_port_of_lading VARCHAR(100),\n",
    "                            weight INT,\n",
    "                            weight_unit VARCHAR(10),\n",
    "                            actual_arrival_date DATE\n",
    "                        )\n",
    "                    END\n",
    "                    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print('Table has been created')\n",
    "except:\n",
    "    print('Alrdy exist or something went wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE HEADER to DATABASE\n",
    "df.to_sql(table_header, engine, if_exists='append', index=False, schema='dbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------ Load Tariff_Cargodesc -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pq_df('tariff_cargodesc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_tfcargo = 'tariff_cargodesc'\n",
    "try:\n",
    "    create_table_query = f\"\"\"\n",
    "                    IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{table_tfcargo}')\n",
    "                    BEGIN\n",
    "                        CREATE TABLE {table_tfcargo} (\n",
    "                            identifier BIGINT,\n",
    "                            container_number VARCHAR(50),\n",
    "                            piece_count DECIMAL(10, 2),\n",
    "                            harmonized_value DECIMAL(15, 2)\n",
    "                        )\n",
    "                    END\n",
    "                    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print('Table has been created')\n",
    "except:\n",
    "    print('Alrdy exist or something went wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE HEADER to DATABASE\n",
    "df.to_sql(table_tfcargo, engine, if_exists='append', index=False, schema='dbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE CASE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1  Frequency for each type of service  - container table \n",
    "query = \"\"\"\n",
    "        SELECT type_of_service, COUNT(*) as service_count\n",
    "        FROM Container\n",
    "        GROUP BY type_of_service\n",
    "        \"\"\"\n",
    "df_svc_type_count = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_svc_type_count.plot(kind='bar', x='type_of_service', y='service_count', legend=None)\n",
    "plt.xlabel('Type of Service')\n",
    "plt.ylabel('Count')\n",
    "plt.yscale('log')     \n",
    "plt.title('Occurrences of Each Type of Service')\n",
    "plt.tight_layout()  \n",
    "for i, v in enumerate(df_svc_type_count['service_count']):\n",
    "    ax.text(i, v + 1, str(v), ha='center', va='bottom', fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# Majority of shipments are customer buying from merchant shipping straight to their doors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 Weight Group proportional to Value for containers? \n",
    "# header : weight  => tot_val\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT \n",
    "            h.weight,\n",
    "            SUM(c.piece_count * c.harmonized_value) AS total_value\n",
    "        FROM \n",
    "            header AS h\n",
    "        JOIN \n",
    "            tariff_cargodesc AS c ON h.identifier = c.identifier\n",
    "        GROUP BY \n",
    "            h.weight\n",
    "        \"\"\"\n",
    "df_val_vs_weight = pd.read_sql_query(query, engine)\n",
    "\n",
    "\n",
    "\n",
    "df_val_vs_weight['weight_bin'] = pd.cut(df_val_vs_weight['weight'], bins=np.arange(0, df_val_vs_weight['weight'].max(), 5000))\n",
    "grouped = df_val_vs_weight.groupby('weight_bin')['total_value'].mean().reset_index()\n",
    "\n",
    "# Plotting the line graph\n",
    "plt.plot(grouped['weight_bin'].astype(str), grouped['total_value'], marker='o')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "plt.yscale('log')  # Apply logarithmic scale to the y-axis\n",
    "plt.xlabel('Weight Bins')\n",
    "plt.ylabel('Average Total Value (log scale)')\n",
    "plt.title('Line Graph of Average Value by Weight Bins')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3  TOTAL weight of cargo for each port?   header\n",
    "query = \"\"\"\n",
    "        SELECT port_of_unlading, SUM(weight) AS total_weight\n",
    "        FROM header\n",
    "        GROUP BY port_of_unlading\n",
    "        ORDER BY total_weight DESC \n",
    "        \"\"\"\n",
    "df_port_weights = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_port_weights.sort_values(by='total_weight', ascending=False)\n",
    "# Most weight unloaded at cali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 : Which ports were shipped from the most?\n",
    "query = \"\"\"\n",
    "        SELECT foreign_port_of_lading, COUNT(*) AS total_appearances\n",
    "        FROM header\n",
    "        GROUP BY foreign_port_of_lading\n",
    "        ORDER BY total_appearances DESC;\n",
    "        \"\"\"\n",
    "df_shipped_port = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shipped_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 : Which ports received the most shipments?\n",
    "query = \"\"\"\n",
    "        SELECT port_of_unlading, COUNT(*) AS total_appearances\n",
    "        FROM header\n",
    "        GROUP BY port_of_unlading\n",
    "        ORDER BY total_appearances DESC;\n",
    "        \"\"\"\n",
    "df_received_port = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_received_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 : Frequency of Container Sizes used?\n",
    "query = \"\"\"\n",
    "        SELECT container_length, container_height, container_width, COUNT(*) AS frequency\n",
    "        FROM container\n",
    "        GROUP BY container_length, container_height, container_width;\n",
    "        \"\"\"\n",
    "df_container_freq = pd.read_sql_query(query, engine)\n",
    "labels = [f'{row.container_length}x{row.container_height}x{row.container_width}' for index, row in df_container_freq.iterrows()]\n",
    "sizes = df_container_freq['frequency'].tolist()\n",
    "\n",
    "# Creating the pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.4f%%', startangle=140)\n",
    "plt.title('Frequency of Container Size Combinations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_container_freq.sort_values(by='frequency', ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 : How frequent were delays?\n",
    "query = \"\"\" \n",
    "        SELECT COUNT(*) AS total_delays\n",
    "        FROM header\n",
    "        WHERE actual_arrival_date > estimated_arrival_date;\n",
    "        \"\"\"\n",
    "df_delay_freq = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 : Which ports had the most delays?\n",
    "query = \"\"\" \n",
    "        SELECT port_of_unlading, COUNT(*) AS delay_count\n",
    "        FROM header\n",
    "        WHERE actual_arrival_date > estimated_arrival_date\n",
    "        GROUP BY port_of_unlading\n",
    "        ORDER BY delay_count DESC;\n",
    "        \"\"\"\n",
    "df_port_delay_freq = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_port_delay_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9 : Which months did these delays occur in?\n",
    "query = \"\"\" \n",
    "        SELECT DATEPART(MONTH, actual_arrival_date) AS delay_month, COUNT(*) AS delay_count\n",
    "        FROM header\n",
    "        WHERE actual_arrival_date > estimated_arrival_date\n",
    "        GROUP BY DATEPART(MONTH, actual_arrival_date)\n",
    "        ORDER BY delay_count DESC;\n",
    "        \"\"\"\n",
    "df_monthly_delay = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly_delay\n",
    "# surprisingly decemeber is not # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
