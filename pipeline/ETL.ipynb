{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT \n",
    "## Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load in modules we will need \"\"\"\n",
    "import os\n",
    "import gc\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime as dt\n",
    "load_dotenv()\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Reading\" from Bronze Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieve all the csv file names from a given path\n",
    "\n",
    "Parameters: \n",
    "path (string)- Path we are looking through\n",
    "\n",
    "Returns: a list of .csv files\n",
    "\"\"\"\n",
    "def grab_files(path: str) -> list:\n",
    "    return [file for file in os.listdir(path) if file.endswith(\".csv\")]\n",
    "\n",
    "\"\"\"\n",
    "Creates DataFrames for all data of a specific entity in \"Bronze layer\"\n",
    "\n",
    "Parameters: \n",
    "path (string) - The \"container\" for the data \n",
    "\n",
    "Returns: a list of .csv files\n",
    "\"\"\"\n",
    "def gen_dfs(ospath = 'BRONZE_LAYER', path:str =''):    \n",
    "    dfs = []\n",
    "    try:\n",
    "        data_dir = f'{os.environ[ospath]}{path + \"/\" if path != \"\" else path}'\n",
    "        files = grab_files(data_dir)\n",
    "        for f in files:\n",
    "            df = pd.read_csv(data_dir+f)\n",
    "            df.name = f.split('__')[1]\n",
    "            dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error with pathing\")\n",
    "    except:\n",
    "        print('Error Occurred while extracting data from the file')\n",
    "    return dfs        #create the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM\n",
    "## Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Will Attempt to Drop all declared columns from a DataFrame\n",
    "\n",
    "Parameters: \n",
    "df - a singular Data Frame\n",
    "cols - a list of strings containing column attributes to be dropped\n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def drop_dfcols(df, cols:list):\n",
    "    try:\n",
    "        df.drop(columns = cols, inplace = True)\n",
    "    except KeyError:\n",
    "        print(f'fail to remove{cols}')\n",
    "\n",
    "\"\"\"\n",
    "Will Attempt to Drop all declared columns from a List of DataFrame\n",
    "\n",
    "Parameters: \n",
    "df - a List of Data Frame\n",
    "cols - a list of strings containing column attributes to be dropped \n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def drop_dfs_col(dfs, cols: list):\n",
    "    try:\n",
    "        for df in dfs:\n",
    "            drop_dfcols(df,cols) \n",
    "    except KeyError:\n",
    "        print('no columns left')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove duplicate records from the DataFrame\n",
    "\"\"\"\n",
    "def rm_dupe(df):\n",
    "    df.drop_duplicates(inplace= True)\n",
    "\n",
    "\"\"\"\n",
    "Drops the columns that have null vals\n",
    "or we fill them will default values if we want\n",
    "\"\"\"\n",
    "def rm_nulls(df, col, default_val = None):\n",
    "    if default_val:\n",
    "        df[col].fillna(default_val, inplace = True)\n",
    "    else:\n",
    "        df.dropna(subset =[col], inplace= True) \n",
    "\n",
    "\"\"\"\n",
    "Checks to see if any column of a DataFrame contains Nulls\n",
    "-best to do this after dropping useless columns [faster]\n",
    "\"\"\"\n",
    "def cols_with_null(df):\n",
    "    has_null = []\n",
    "    for col in df.columns:\n",
    "        print(f'{col} : {df[col].isnull().values.any()}')\n",
    "        if df[col].isnull().values.any():\n",
    "            has_null.append(col)\n",
    "    return has_null\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "dfs: List of DataFrames \n",
    "Return: Merged DataFrames \n",
    "\"\"\"\n",
    "def merge_dfs(dfs: list):\n",
    "    name_attr = dfs[0].name if hasattr(dfs[0], \"name\") else None\n",
    "    dfs = pd.concat(dfs, ignore_index=True)        #put all the data into one table\n",
    "    if name_attr:\n",
    "        dfs.name = name_attr\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checks to see if a val is of type : etype\n",
    "\n",
    "Parameters:\n",
    "etype: Type \n",
    "\n",
    "Returns:\n",
    "Boolean if it is that type\n",
    "\"\"\"\n",
    "def is_type(val,  etype):\n",
    "    try:\n",
    "        etype(val)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "    \n",
    "\"\"\"\n",
    "Removes all the records that have an invalid type\n",
    "\n",
    "Parameters:\n",
    "header_df : DataFrame\n",
    "col : column name to check\n",
    "etype: Type \n",
    "\n",
    "Returns:\n",
    "DataFrame with records that have valid type\n",
    "\"\"\"\n",
    "def rm_inval_type(df, col, etype):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[df[col].apply(lambda val: is_type(val,etype))]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Removes outliers (using Interquartile range) from a given column\n",
    "Parameters:\n",
    "df: DataFrame\n",
    "col: column to check for outliers\n",
    "Returns: DataFrame with outliers removed\n",
    "\"\"\"\n",
    "def IQR_outlier(df, col: str):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    Q1 = df[col].quantile(0.25)   \n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    # non_outliers = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    df_rm_outliers = df[~((df[col] < lower_bound) | (df[col] > upper_bound))]\n",
    "    if name_attr:\n",
    "        df_rm_outliers.name = name_attr\n",
    "    return df_rm_outliers\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Removes outliers (using Z-score) from a given column\n",
    "Parameters:\n",
    "df: DataFrame\n",
    "col: column to check for outliers\n",
    "deviance: outlier bounds\n",
    "Returns: DataFrame with outliers removed\n",
    "\"\"\"\n",
    "def zscore(df, col:str, deviance:int):\n",
    "    col_mean = df[col].mean()\n",
    "    col_std = df[col].std()\n",
    "    return df[abs((df[col] - col_mean)/ col_std) <= deviance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD\n",
    "### Save into Currated layer [Silver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Will file \"modded_{filename}.csv\"\n",
    "in curated-layer directory.\n",
    "If path does not exist, will create it for you.\n",
    "\n",
    "Parameters: \n",
    "df - a Data Frame\n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def df2csv(df):\n",
    "    year = df.name[-4:]\n",
    "    type = df.name[:-5]\n",
    "    path = f'../data_set/CBP_AMS/curated-layer/{year}/{type}/'          \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    df.to_csv(path + f'modded_{df.name}.csv', sep='|' , index=False) \n",
    "\n",
    "    \"\"\"\n",
    "Will file \"modded_{filename}.parquet\"\n",
    "in curated-layer directory.\n",
    "If path does not exist, will create it for you.\n",
    "\n",
    "Parameters: \n",
    "df - a Data Frame\n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def df2parquet(df):\n",
    "    year = df.name[-4:]\n",
    "    type = df.name[:-5]\n",
    "    path = f'../data_set/CBP_AMS/curated-layer/{year}/{type}/'          \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    df.to_parquet(path + f'modded_{df.name}.parquet', engine='pyarrow',index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Current Use Case : We can reduce data overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_billgen = ['house_bol_number','sub_house_bol_number', 'bill_type_code',\n",
    "              'manifest_number', 'trade_update_date', 'run_date']\n",
    "\n",
    "rm_container = ['seal_number_1', 'seal_number_2',\n",
    "       'equipment_description_code','container_type', 'load_status']\n",
    "\n",
    "rm_header  = ['carrier_code', 'vessel_country_code', 'vessel_name',\n",
    "       'foreign_port_of_lading_qualifier', 'manifest_quantity', 'manifest_unit', \n",
    "       'measurement', 'measurement_unit', 'record_status_indicator',\n",
    "       'place_of_receipt', 'port_of_destination', 'foreign_port_of_destination_qualifier', \n",
    "       'foreign_port_of_destination', 'conveyance_id_qualifier', 'conveyance_id', \n",
    "       'in_bond_entry_type', 'mode_of_transportation', 'secondary_notify_party_1',\n",
    "       'secondary_notify_party_2', 'secondary_notify_party_3', 'secondary_notify_party_4', \n",
    "       'secondary_notify_party_5', 'secondary_notify_party_6', 'secondary_notify_party_7',\n",
    "       'secondary_notify_party_8', 'secondary_notify_party_9', 'secondary_notify_party_10']\n",
    "\n",
    "rm_tariff = ['description_sequence_number','harmonized_number', 'harmonized_weight_unit','harmonized_weight']\n",
    "\n",
    "rm_cargodesc = [ 'description_sequence_number','description_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------HEADER--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Header Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert the weight column to a universal weight unit measurement\n",
    "\n",
    "Parameters:\n",
    "header_df: DataFrame\n",
    "\n",
    "Returns:\n",
    "None\n",
    "\"\"\"\n",
    "def universal_unit(header_df):\n",
    "    unit_weight = {\n",
    "        'Kilograms': 1.0,\n",
    "        'Pounds': 0.453592, \n",
    "        'Metric Ton': 1000.0, \n",
    "        'Long Ton': 1016.04691, \n",
    "        'Measurement Ton': 1.01604691\n",
    "    }\n",
    "    # Convert weight to kg based on weight_unit\n",
    "    for unit, scale in unit_weight.items():\n",
    "        header_df.loc[header_df['weight_unit'] == unit, 'weight'] *= scale\n",
    "        header_df['weight_unit'] = 'Kilograms'\n",
    "\n",
    "\n",
    "def is_date(val):\n",
    "    try:\n",
    "        dt.strptime(val, '%Y-%m-%d')\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def rm_inval_date(df,col):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[df[col].apply(is_date)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "removes the records that have dates that are not in the scope of this year\n",
    "\"\"\"\n",
    "def date_outlier(df):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    year = int(df.name[-4:])\n",
    "    df['estimated_arrival_date'] = pd.to_datetime(df['estimated_arrival_date'], format='%Y-%m-%d')\n",
    "    df['actual_arrival_date'] = pd.to_datetime(df['actual_arrival_date'], format='%Y-%m-%d')\n",
    "    df = df[df['actual_arrival_date'].dt.year <= year]   # we are not in the future!!!\n",
    "    df = df[(abs(df['estimated_arrival_date'].dt.year - year) <= 5)]\n",
    "    df = df[(abs(df['actual_arrival_date'].dt.year - year) <= 5)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in HEADER Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   \n",
    "dfs = gen_dfs(path='header')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(dfs, rm_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one \n",
    "dfs = merge_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records\n",
    "rm_dupe(dfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of any irregular data in the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure there are no null values 😀\n",
    "for col in cols_with_null(dfs):\n",
    "    rm_nulls(dfs, col)\n",
    "# Header does not need to deal with any Nulls 🎉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of weight columns \n",
    "dfs = rm_inval_type(dfs, 'weight', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of port\n",
    "dfs = rm_inval_type(dfs, 'port_of_unlading', str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of port\n",
    "dfs = rm_inval_type(dfs, 'foreign_port_of_lading', str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of unit\n",
    "dfs = rm_inval_type(dfs, 'weight_unit', str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid date in this col\n",
    "dfs = rm_inval_date(dfs, 'estimated_arrival_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid date in this col\n",
    "dfs = rm_inval_date(dfs, 'actual_arrival_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove out the Outliers\n",
    "dfs = IQR_outlier(dfs, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any date outliers \n",
    "dfs = date_outlier(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the weight unit to a singular unit ( Kilogram )\n",
    "universal_unit(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned up Header Data -> Store into \"Silver Layer\" \n",
    "df2parquet(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------BILL GEN--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   \n",
    "dfs = gen_dfs(path='billgen')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(dfs, rm_billgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one  \n",
    "dfs = merge_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records  \n",
    "rm_dupe(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any null values\n",
    "for col in cols_with_null(dfs):\n",
    "    rm_nulls(dfs, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned up Bill Gen Data -> Store into \"Silver Layer\" \n",
    "df2parquet(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------CONTAINER--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_box_outlier(df):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[(df['container_length'] != 0) & (df['container_width'] != 0) & (df['container_height'] != 0)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   \n",
    "dfs = gen_dfs(path='container')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(dfs, rm_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one    30635677 rows × 6 columns   27234627 rows × 6 columns\n",
    "dfs = merge_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records  \n",
    "rm_dupe(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any null values\n",
    "for col in cols_with_null(dfs):\n",
    "    rm_nulls(dfs, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid length in this col\n",
    "dfs = rm_inval_type(dfs, 'container_length', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid height in this col\n",
    "dfs = rm_inval_type(dfs, 'container_height', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any invalid width in this col\n",
    "dfs = rm_inval_type(dfs, 'container_width', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = rm_box_outlier(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_counts = dfs.groupby(['container_length', 'container_height', 'container_width']).size().reset_index(name='counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_counts.sort_values('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2parquet(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------Tariff + Cargo Desc------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Tariff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_cgd_outliers(df):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[(df['piece_count'] != 0)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df\n",
    "\n",
    "def rm_tariff_outliers(df):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    df = df[(df['harmonized_value'] > 0)]\n",
    "    if name_attr:\n",
    "        df.name = name_attr\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   |   cargodesc\n",
    "tdfs = gen_dfs(path='tariff')\n",
    "tdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(tdfs, rm_tariff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one   \n",
    "tdfs = merge_dfs(tdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records  \n",
    "rm_dupe(tdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any null values\n",
    "for col in cols_with_null(tdfs):\n",
    "    rm_nulls(tdfs, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of piece\n",
    "tdfs = rm_inval_type(tdfs,'harmonized_value', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove value outliers\n",
    "tdfs = rm_tariff_outliers(tdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdfs = IQR_outlier(tdfs, 'harmonized_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Cargo Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   |   cargodesc\n",
    "dfs = gen_dfs(path='cargodesc')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(dfs, rm_cargodesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one    30635677 rows × 6 columns   27234627 rows × 6 columns\n",
    "dfs = merge_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records  \n",
    "rm_dupe(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any null values\n",
    "for col in cols_with_null(dfs):\n",
    "    rm_nulls(dfs, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type of piece\n",
    "dfs = rm_inval_type(dfs,'piece_count', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = rm_cgd_outliers(dfs)\n",
    "# done cleaning cargo desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdfs = pd.merge(dfs,tdfs, on=['identifier', 'container_number'])\n",
    "mdfs.name = tdfs.name[:-5] + '_' + dfs.name[:-5] + '_' + tdfs.name[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdfs['tot_cost'] = mdfs['piece_count'] * mdfs['harmonized_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdfs.sort_values('container_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2parquet(mdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Data Into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "def grab_files(path: str) -> list:\n",
    "    file=[file for file in os.listdir(path) if file.endswith(\".parquet\")]\n",
    "    return file[0]\n",
    "\n",
    "def load_pq_df(model):\n",
    "    path = f'{os.environ['SILVER_LAYER']}{model}/'\n",
    "    file = grab_files(path)\n",
    "    df = pd.read_parquet(path + file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = urllib.parse.quote_plus(r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                                 r'SERVER=DESKTOP-N9SA336;'\n",
    "                                 r'DATABASE=BastionSLA;'\n",
    "                                 r'Trusted_Connection=yes;')\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};Server={DESKTOP-N9SA336};Database=;Trusted_Connection=yes;')\n",
    "cursor = conn.cursor() # Cursors are database level objects that let your query a database multiple times - Think of it as a pointer to a row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Bastion Analytic DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyodbc.Cursor at 0x1f4db8696b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DB as Silver Layer Storage\n",
    "db_name = \"BastionSLA\"\n",
    "cursor.execute(f\"\"\"\n",
    "        IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '{db_name}')\n",
    "        BEGIN\n",
    "            CREATE DATABASE {db_name}\n",
    "        END\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyodbc.Cursor at 0x1f4db8696b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('use BastionSLA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Bill Gen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pq_df('billgen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has been created\n"
     ]
    }
   ],
   "source": [
    "table_billgen = 'billgen'\n",
    "try:\n",
    "    create_table_query = f\"\"\"\n",
    "                    IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{table_billgen}')\n",
    "                    BEGIN\n",
    "                        CREATE TABLE {table_billgen} (\n",
    "                            identifier BIGINT NOT NULL,\n",
    "                            master_bol_number VARCHAR(255),\n",
    "                            voyage_number VARCHAR(50)\n",
    "                        )\n",
    "                    END\n",
    "                    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print('Table has been created')\n",
    "except:\n",
    "    print('Alrdy exist or something went wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE BILLGEN to DATABASE\n",
    "df.to_sql(table_billgen, engine, if_exists='append', index=False, schema='dbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------ Load Container -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pq_df('container')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has been created\n"
     ]
    }
   ],
   "source": [
    "table_container = 'container'\n",
    "try:\n",
    "    create_table_query = f\"\"\"\n",
    "                    IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{table_container}')\n",
    "                    BEGIN\n",
    "                        CREATE TABLE {table_container} (\n",
    "                            identifier BIGINT, \n",
    "                            container_number VARCHAR(50), \n",
    "                            container_length INT,\n",
    "                            container_height INT,\n",
    "                            container_width INT,\n",
    "                            type_of_service VARCHAR(255) \n",
    "                        )\n",
    "                    END\n",
    "                    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print('Table has been created')\n",
    "except:\n",
    "    print('Alrdy exist or something went wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE BILLGEN to DATABASE\n",
    "df.to_sql(table_container, engine, if_exists='append', index=False, schema='dbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------ Load Header -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pq_df('header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has been created\n"
     ]
    }
   ],
   "source": [
    "table_header = 'header'\n",
    "try:\n",
    "    create_table_query = f\"\"\"\n",
    "                    IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{table_header}')\n",
    "                    BEGIN\n",
    "                        CREATE TABLE {table_header} (\n",
    "                            identifier BIGINT,\n",
    "                            port_of_unlading VARCHAR(100),\n",
    "                            estimated_arrival_date DATE,\n",
    "                            foreign_port_of_lading VARCHAR(100),\n",
    "                            weight INT,\n",
    "                            weight_unit VARCHAR(10),\n",
    "                            actual_arrival_date DATE\n",
    "                        )\n",
    "                    END\n",
    "                    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print('Table has been created')\n",
    "except:\n",
    "    print('Alrdy exist or something went wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE HEADER to DATABASE\n",
    "df.to_sql(table_header, engine, if_exists='append', index=False, schema='dbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------ Load Tariff_Cargodesc -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pq_df('tariff_cargodesc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has been created\n"
     ]
    }
   ],
   "source": [
    "table_tfcargo = 'tariff_cargodesc'\n",
    "try:\n",
    "    create_table_query = f\"\"\"\n",
    "                    IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{table_tfcargo}')\n",
    "                    BEGIN\n",
    "                        CREATE TABLE {table_tfcargo} (\n",
    "                            identifier BIGINT,\n",
    "                            container_number VARCHAR(50),\n",
    "                            piece_count DECIMAL(10, 2),\n",
    "                            harmonized_value DECIMAL(15, 2),\n",
    "                            tot_cost DECIMAL(18, 2) \n",
    "                        )\n",
    "                    END\n",
    "                    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print('Table has been created')\n",
    "except:\n",
    "    print('Alrdy exist or something went wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>container_number</th>\n",
       "      <th>piece_count</th>\n",
       "      <th>harmonized_value</th>\n",
       "      <th>tot_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201901012019</td>\n",
       "      <td>CMAU5762030</td>\n",
       "      <td>26.000</td>\n",
       "      <td>7660.000</td>\n",
       "      <td>199160.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201901012019</td>\n",
       "      <td>BEAU4153622</td>\n",
       "      <td>29.000</td>\n",
       "      <td>7660.000</td>\n",
       "      <td>222140.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201901012257</td>\n",
       "      <td>MEDU6656025</td>\n",
       "      <td>22.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>22.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201901012257</td>\n",
       "      <td>MEDU6171826</td>\n",
       "      <td>22.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>22.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201901012257</td>\n",
       "      <td>FCIU4371458</td>\n",
       "      <td>22.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>22.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10957804</th>\n",
       "      <td>2019123164710</td>\n",
       "      <td>TGHU8106362</td>\n",
       "      <td>112.000</td>\n",
       "      <td>5462.000</td>\n",
       "      <td>611744.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10957805</th>\n",
       "      <td>2019123164710</td>\n",
       "      <td>SUDU8572228</td>\n",
       "      <td>112.000</td>\n",
       "      <td>5128.000</td>\n",
       "      <td>574336.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10957806</th>\n",
       "      <td>2019123164782</td>\n",
       "      <td>MNBU4019353</td>\n",
       "      <td>21.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>840.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10957807</th>\n",
       "      <td>2019123164829</td>\n",
       "      <td>NC</td>\n",
       "      <td>1.000</td>\n",
       "      <td>148.000</td>\n",
       "      <td>148.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10957808</th>\n",
       "      <td>2019123164850</td>\n",
       "      <td>BSIU9760531</td>\n",
       "      <td>1764.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1764.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10957809 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             identifier container_number  piece_count  harmonized_value  \\\n",
       "0          201901012019      CMAU5762030       26.000          7660.000   \n",
       "1          201901012019      BEAU4153622       29.000          7660.000   \n",
       "2          201901012257      MEDU6656025       22.000             1.000   \n",
       "3          201901012257      MEDU6171826       22.000             1.000   \n",
       "4          201901012257      FCIU4371458       22.000             1.000   \n",
       "...                 ...              ...          ...               ...   \n",
       "10957804  2019123164710      TGHU8106362      112.000          5462.000   \n",
       "10957805  2019123164710      SUDU8572228      112.000          5128.000   \n",
       "10957806  2019123164782      MNBU4019353       21.000            40.000   \n",
       "10957807  2019123164829               NC        1.000           148.000   \n",
       "10957808  2019123164850      BSIU9760531     1764.000             1.000   \n",
       "\n",
       "           tot_cost  \n",
       "0        199160.000  \n",
       "1        222140.000  \n",
       "2            22.000  \n",
       "3            22.000  \n",
       "4            22.000  \n",
       "...             ...  \n",
       "10957804 611744.000  \n",
       "10957805 574336.000  \n",
       "10957806    840.000  \n",
       "10957807    148.000  \n",
       "10957808   1764.000  \n",
       "\n",
       "[10957809 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE HEADER to DATABASE\n",
    "df.to_sql(table_tfcargo, engine, if_exists='append', index=False, schema='dbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE CASE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1  Frequency for each type of service  - container table \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
