{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT \n",
    "## Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load in modules we will need \"\"\"\n",
    "import os\n",
    "import gc\n",
    "import pymssql\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Reading\" from Bronze Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieve all the csv file names from a given path\n",
    "\n",
    "Parameters: \n",
    "path (string)- Path we are looking through\n",
    "\n",
    "Returns: a list of .csv files\n",
    "\"\"\"\n",
    "def grab_files(path: str) -> list:\n",
    "    return [file for file in os.listdir(path) if file.endswith(\".csv\")]\n",
    "\n",
    "\"\"\"\n",
    "Creates DataFrames for all data of a specific entity in \"Bronze layer\"\n",
    "\n",
    "Parameters: \n",
    "path (string) - The \"container\" for the data \n",
    "\n",
    "Returns: a list of .csv files\n",
    "\"\"\"\n",
    "def gen_dfs(ospath = 'BRONZE_LAYER', path:str =''):    \n",
    "    dfs = []\n",
    "    try:\n",
    "        data_dir = f'{os.environ[ospath]}{path + \"/\" if path != \"\" else path}'\n",
    "        files = grab_files(data_dir)\n",
    "        for f in files:\n",
    "            df = pd.read_csv(data_dir+f)\n",
    "            df.name = f.split('__')[1]\n",
    "            dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error with pathing\")\n",
    "    except:\n",
    "        print('Error Occurred while extracting data from the file')\n",
    "    return dfs        #create the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM\n",
    "## Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Will Attempt to Drop all declared columns from a DataFrame\n",
    "\n",
    "Parameters: \n",
    "df - a singular Data Frame\n",
    "cols - a list of strings containing column attributes to be dropped\n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def drop_dfcols(df, cols:list):\n",
    "    try:\n",
    "        df.drop(columns = cols, inplace = True)\n",
    "    except KeyError:\n",
    "        print(f'fail to remove{cols}')\n",
    "\n",
    "\"\"\"\n",
    "Will Attempt to Drop all declared columns from a List of DataFrame\n",
    "\n",
    "Parameters: \n",
    "df - a List of Data Frame\n",
    "cols - a list of strings containing column attributes to be dropped \n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def drop_dfs_col(dfs, cols: list):\n",
    "    try:\n",
    "        for df in dfs:\n",
    "            drop_dfcols(df,cols) \n",
    "    except KeyError:\n",
    "        print('no columns left')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove duplicate records from the DataFrame\n",
    "\"\"\"\n",
    "def rm_dupe(df):\n",
    "    df.drop_duplicates(inplace= True)\n",
    "\n",
    "\"\"\"\n",
    "Drops the columns that have null vals\n",
    "or we fill them will default values if we want\n",
    "\"\"\"\n",
    "def rm_nulls(df, col, default_val = None):\n",
    "    if default_val:\n",
    "        df[col].fillna(default_val, inplace = True)\n",
    "    else:\n",
    "        df.dropna(subset =[col], inplace= True) \n",
    "\n",
    "\"\"\"\n",
    "Checks to see if any column of a DataFrame contains Nulls\n",
    "-best to do this after dropping useless columns [faster]\n",
    "\"\"\"\n",
    "def cols_with_null(df):\n",
    "    for col in df.columns:\n",
    "        print(f'{col} : {df[col].isnull().values.any()}')\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "dfs: List of DataFrames \n",
    "Return: Merged DataFrames \n",
    "\"\"\"\n",
    "def merge_dfs(dfs: list):\n",
    "    name_attr = dfs[0].name if hasattr(dfs[0], \"name\") else None\n",
    "    dfs = pd.concat(dfs, ignore_index=True)        #put all the data into one table\n",
    "    if name_attr:\n",
    "        dfs.name = name_attr\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checks to see if a val is of type : etype\n",
    "\n",
    "Parameters:\n",
    "etype: Type \n",
    "\n",
    "Returns:\n",
    "Boolean if it is that type\n",
    "\"\"\"\n",
    "def is_type(val,  etype):\n",
    "    try:\n",
    "        etype(val)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "    \n",
    "\"\"\"\n",
    "Removes all the records that have an invalid type\n",
    "\n",
    "Parameters:\n",
    "header_df : DataFrame\n",
    "col : column name to check\n",
    "etype: Type \n",
    "\n",
    "Returns:\n",
    "DataFrame with records that have valid type\n",
    "\"\"\"\n",
    "def rm_inval_type(df, col, etype):\n",
    "    return df[df[col].apply(lambda val: is_type(val,etype))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IQR_outlier(df, col: str):\n",
    "    name_attr = df.name if hasattr(df, \"name\") else None\n",
    "    Q1 = df[col].quantile(0.25)   \n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    # non_outliers = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    df_rm_outliers = df[~((df[col] < lower_bound) | (df[col] > upper_bound))]\n",
    "    if name_attr:\n",
    "        df_rm_outliers.name = name_attr\n",
    "    return df_rm_outliers\n",
    "\n",
    "def zscore(df, col:str, deviance:int):\n",
    "    col_mean = df[col].mean()\n",
    "    col_std = df[col].std()\n",
    "    return df[abs((df[col] - col_mean)/ col_std) <= deviance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD\n",
    "### Save into Currated layer [Silver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Will file \"modded_{filename}.csv\"\n",
    "in curated-layer directory.\n",
    "If path does not exist, will create it for you.\n",
    "\n",
    "Parameters: \n",
    "df - a Data Frame\n",
    "\n",
    "Returns: Nothing\n",
    "\"\"\"\n",
    "def df2csv(df):\n",
    "    year = df.name[-4:]\n",
    "    type = df.name[:-5]\n",
    "    path = f'../data_set/CBP_AMS/curated-layer/{year}/{type}/'          \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    df.to_csv(path + f'modded_{df.name}.csv', sep='|' , index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Current Use Case : We can reduce data overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_billgen = ['house_bol_number','sub_house_bol_number', 'bill_type_code',\n",
    "              'manifest_number', 'trade_update_date', 'run_date']\n",
    "\n",
    "rm_container = ['seal_number_1', 'seal_number_2',\n",
    "       'equipment_description_code','container_type', 'load_status']\n",
    "\n",
    "rm_header  = ['carrier_code', 'vessel_country_code', 'vessel_name',\n",
    "       'foreign_port_of_lading_qualifier', 'manifest_quantity', 'manifest_unit', \n",
    "       'measurement', 'measurement_unit', 'record_status_indicator',\n",
    "       'place_of_receipt', 'port_of_destination', 'foreign_port_of_destination_qualifier', \n",
    "       'foreign_port_of_destination', 'conveyance_id_qualifier', 'conveyance_id', \n",
    "       'in_bond_entry_type', 'mode_of_transportation', 'secondary_notify_party_1',\n",
    "       'secondary_notify_party_2', 'secondary_notify_party_3', 'secondary_notify_party_4', \n",
    "       'secondary_notify_party_5', 'secondary_notify_party_6', 'secondary_notify_party_7',\n",
    "       'secondary_notify_party_8', 'secondary_notify_party_9', 'secondary_notify_party_10']\n",
    "\n",
    "rm_tariff = ['description_sequence_number','harmonized_number', 'harmonized_weight_unit','harmonized_weight']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------HEADER--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Header Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert the weight column to a universal weight unit measurement\n",
    "\n",
    "Parameters:\n",
    "header_df: DataFrame\n",
    "\n",
    "Returns:\n",
    "None\n",
    "\"\"\"\n",
    "def universal_unit(header_df):\n",
    "    unit_weight = {\n",
    "        'Kilograms': 1.0,\n",
    "        'Pounds': 0.453592, \n",
    "        'Metric Ton': 1000.0, \n",
    "        'Long Ton': 1016.04691, \n",
    "        'Measurement Ton': 1.01604691\n",
    "    }\n",
    "    # Convert weight to kg based on weight_unit\n",
    "    for unit, scale in unit_weight.items():\n",
    "        header_df.loc[header_df['weight_unit'] == unit, 'weight'] *= scale\n",
    "        header_df['weight_unit'] = 'Kilograms'\n",
    "\n",
    "\"\"\"\n",
    "removes the records that have dates that are not in the scope of this year\n",
    "\"\"\"\n",
    "def date_outlier(header_df):\n",
    "    year = header_df.name[-4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in HEADER Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvin\\AppData\\Local\\Temp\\ipykernel_23104\\3507972516.py:26: DtypeWarning: Columns (25,26,27,28,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_dir+f)\n",
      "C:\\Users\\alvin\\AppData\\Local\\Temp\\ipykernel_23104\\3507972516.py:26: DtypeWarning: Columns (25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_dir+f)\n",
      "C:\\Users\\alvin\\AppData\\Local\\Temp\\ipykernel_23104\\3507972516.py:26: DtypeWarning: Columns (25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_dir+f)\n",
      "C:\\Users\\alvin\\AppData\\Local\\Temp\\ipykernel_23104\\3507972516.py:26: DtypeWarning: Columns (25,26,27,28,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_dir+f)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>carrier_code</th>\n",
       "      <th>vessel_country_code</th>\n",
       "      <th>vessel_name</th>\n",
       "      <th>port_of_unlading</th>\n",
       "      <th>estimated_arrival_date</th>\n",
       "      <th>foreign_port_of_lading_qualifier</th>\n",
       "      <th>foreign_port_of_lading</th>\n",
       "      <th>manifest_quantity</th>\n",
       "      <th>manifest_unit</th>\n",
       "      <th>...</th>\n",
       "      <th>secondary_notify_party_2</th>\n",
       "      <th>secondary_notify_party_3</th>\n",
       "      <th>secondary_notify_party_4</th>\n",
       "      <th>secondary_notify_party_5</th>\n",
       "      <th>secondary_notify_party_6</th>\n",
       "      <th>secondary_notify_party_7</th>\n",
       "      <th>secondary_notify_party_8</th>\n",
       "      <th>secondary_notify_party_9</th>\n",
       "      <th>secondary_notify_party_10</th>\n",
       "      <th>actual_arrival_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201901010</td>\n",
       "      <td>OOLU</td>\n",
       "      <td>FR</td>\n",
       "      <td>CMA CGM FIGARO</td>\n",
       "      <td>Oakland, California</td>\n",
       "      <td>2017-10-08</td>\n",
       "      <td>Schedule K Foreign Port</td>\n",
       "      <td>Tanjung Priok,Indonesia</td>\n",
       "      <td>834</td>\n",
       "      <td>PCS</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201901011</td>\n",
       "      <td>AEHS</td>\n",
       "      <td>JP</td>\n",
       "      <td>MOL COMMITMENT</td>\n",
       "      <td>Los Angeles, California</td>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>Schedule K Foreign Port</td>\n",
       "      <td>Singapore,Singapore</td>\n",
       "      <td>60</td>\n",
       "      <td>PKG</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201901012</td>\n",
       "      <td>EXDO</td>\n",
       "      <td>PA</td>\n",
       "      <td>EVER LUCKY</td>\n",
       "      <td>Norfolk, Virginia</td>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>Schedule K Foreign Port</td>\n",
       "      <td>Rotterdam,Netherlands</td>\n",
       "      <td>3350</td>\n",
       "      <td>PKG</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201901013</td>\n",
       "      <td>EXDO</td>\n",
       "      <td>PA</td>\n",
       "      <td>EVER LUCKY</td>\n",
       "      <td>Norfolk, Virginia</td>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>Schedule K Foreign Port</td>\n",
       "      <td>Bremerhaven,Federal Republic of Germany</td>\n",
       "      <td>642</td>\n",
       "      <td>PKG</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201901014</td>\n",
       "      <td>EXDO</td>\n",
       "      <td>PA</td>\n",
       "      <td>EVER LUCKY</td>\n",
       "      <td>Norfolk, Virginia</td>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>Schedule K Foreign Port</td>\n",
       "      <td>Bremerhaven,Federal Republic of Germany</td>\n",
       "      <td>2743</td>\n",
       "      <td>CTN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999995</th>\n",
       "      <td>2019040621908</td>\n",
       "      <td>FTNV</td>\n",
       "      <td>HK</td>\n",
       "      <td>MOL BRILLIANCE</td>\n",
       "      <td>Long Beach, California</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>Schedule K Foreign Port</td>\n",
       "      <td>Hong Kong,Hong Kong</td>\n",
       "      <td>55</td>\n",
       "      <td>CTN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999996</th>\n",
       "      <td>2019040621909</td>\n",
       "      <td>ONEY</td>\n",
       "      <td>HK</td>\n",
       "      <td>MOL BRILLIANCE</td>\n",
       "      <td>Long Beach, California</td>\n",
       "      <td>2019-04-04</td>\n",
       "      <td>Schedule K Foreign Port</td>\n",
       "      <td>Hong Kong,Hong Kong</td>\n",
       "      <td>1260</td>\n",
       "      <td>PKG</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999997</th>\n",
       "      <td>2019040621910</td>\n",
       "      <td>HYSL</td>\n",
       "      <td>HK</td>\n",
       "      <td>MOL BRILLIANCE</td>\n",
       "      <td>Long Beach, California</td>\n",
       "      <td>2019-04-02</td>\n",
       "      <td>Schedule K Foreign Port</td>\n",
       "      <td>Yantian,China (Mainland)</td>\n",
       "      <td>825</td>\n",
       "      <td>CTN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999998</th>\n",
       "      <td>2019040621911</td>\n",
       "      <td>HEIC</td>\n",
       "      <td>PA</td>\n",
       "      <td>BANGKOK BRIDGE</td>\n",
       "      <td>Long Beach, California</td>\n",
       "      <td>2019-04-03</td>\n",
       "      <td>Schedule K Foreign Port</td>\n",
       "      <td>Kobe,Japan</td>\n",
       "      <td>590</td>\n",
       "      <td>CTN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999999</th>\n",
       "      <td>2019040621912</td>\n",
       "      <td>ONEY</td>\n",
       "      <td>HK</td>\n",
       "      <td>MOL BRILLIANCE</td>\n",
       "      <td>Long Beach, California</td>\n",
       "      <td>2019-04-04</td>\n",
       "      <td>Schedule K Foreign Port</td>\n",
       "      <td>Hong Kong,Hong Kong</td>\n",
       "      <td>968</td>\n",
       "      <td>PKG</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000000 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            identifier carrier_code vessel_country_code     vessel_name  \\\n",
       "0            201901010         OOLU                  FR  CMA CGM FIGARO   \n",
       "1            201901011         AEHS                  JP  MOL COMMITMENT   \n",
       "2            201901012         EXDO                  PA      EVER LUCKY   \n",
       "3            201901013         EXDO                  PA      EVER LUCKY   \n",
       "4            201901014         EXDO                  PA      EVER LUCKY   \n",
       "...                ...          ...                 ...             ...   \n",
       "4999995  2019040621908         FTNV                  HK  MOL BRILLIANCE   \n",
       "4999996  2019040621909         ONEY                  HK  MOL BRILLIANCE   \n",
       "4999997  2019040621910         HYSL                  HK  MOL BRILLIANCE   \n",
       "4999998  2019040621911         HEIC                  PA  BANGKOK BRIDGE   \n",
       "4999999  2019040621912         ONEY                  HK  MOL BRILLIANCE   \n",
       "\n",
       "                port_of_unlading estimated_arrival_date  \\\n",
       "0            Oakland, California             2017-10-08   \n",
       "1        Los Angeles, California             2018-01-28   \n",
       "2              Norfolk, Virginia             2018-03-28   \n",
       "3              Norfolk, Virginia             2018-03-28   \n",
       "4              Norfolk, Virginia             2018-03-28   \n",
       "...                          ...                    ...   \n",
       "4999995   Long Beach, California             2019-04-01   \n",
       "4999996   Long Beach, California             2019-04-04   \n",
       "4999997   Long Beach, California             2019-04-02   \n",
       "4999998   Long Beach, California             2019-04-03   \n",
       "4999999   Long Beach, California             2019-04-04   \n",
       "\n",
       "        foreign_port_of_lading_qualifier  \\\n",
       "0                Schedule K Foreign Port   \n",
       "1                Schedule K Foreign Port   \n",
       "2                Schedule K Foreign Port   \n",
       "3                Schedule K Foreign Port   \n",
       "4                Schedule K Foreign Port   \n",
       "...                                  ...   \n",
       "4999995          Schedule K Foreign Port   \n",
       "4999996          Schedule K Foreign Port   \n",
       "4999997          Schedule K Foreign Port   \n",
       "4999998          Schedule K Foreign Port   \n",
       "4999999          Schedule K Foreign Port   \n",
       "\n",
       "                          foreign_port_of_lading  manifest_quantity  \\\n",
       "0                        Tanjung Priok,Indonesia                834   \n",
       "1                            Singapore,Singapore                 60   \n",
       "2                          Rotterdam,Netherlands               3350   \n",
       "3        Bremerhaven,Federal Republic of Germany                642   \n",
       "4        Bremerhaven,Federal Republic of Germany               2743   \n",
       "...                                          ...                ...   \n",
       "4999995                      Hong Kong,Hong Kong                 55   \n",
       "4999996                      Hong Kong,Hong Kong               1260   \n",
       "4999997                 Yantian,China (Mainland)                825   \n",
       "4999998                               Kobe,Japan                590   \n",
       "4999999                      Hong Kong,Hong Kong                968   \n",
       "\n",
       "        manifest_unit  ...  secondary_notify_party_2 secondary_notify_party_3  \\\n",
       "0                 PCS  ...                       NaN                      NaN   \n",
       "1                 PKG  ...                       NaN                      NaN   \n",
       "2                 PKG  ...                       NaN                      NaN   \n",
       "3                 PKG  ...                       NaN                      NaN   \n",
       "4                 CTN  ...                       NaN                      NaN   \n",
       "...               ...  ...                       ...                      ...   \n",
       "4999995           CTN  ...                       NaN                      NaN   \n",
       "4999996           PKG  ...                       NaN                      NaN   \n",
       "4999997           CTN  ...                       NaN                      NaN   \n",
       "4999998           CTN  ...                       NaN                      NaN   \n",
       "4999999           PKG  ...                       NaN                      NaN   \n",
       "\n",
       "         secondary_notify_party_4 secondary_notify_party_5  \\\n",
       "0                             NaN                      NaN   \n",
       "1                             NaN                      NaN   \n",
       "2                             NaN                      NaN   \n",
       "3                             NaN                      NaN   \n",
       "4                             NaN                      NaN   \n",
       "...                           ...                      ...   \n",
       "4999995                       NaN                      NaN   \n",
       "4999996                       NaN                      NaN   \n",
       "4999997                       NaN                      NaN   \n",
       "4999998                       NaN                      NaN   \n",
       "4999999                       NaN                      NaN   \n",
       "\n",
       "        secondary_notify_party_6 secondary_notify_party_7  \\\n",
       "0                            NaN                      NaN   \n",
       "1                            NaN                      NaN   \n",
       "2                            NaN                      NaN   \n",
       "3                            NaN                      NaN   \n",
       "4                            NaN                      NaN   \n",
       "...                          ...                      ...   \n",
       "4999995                      NaN                      NaN   \n",
       "4999996                      NaN                      NaN   \n",
       "4999997                      NaN                      NaN   \n",
       "4999998                      NaN                      NaN   \n",
       "4999999                      NaN                      NaN   \n",
       "\n",
       "        secondary_notify_party_8 secondary_notify_party_9  \\\n",
       "0                            NaN                      NaN   \n",
       "1                            NaN                      NaN   \n",
       "2                            NaN                      NaN   \n",
       "3                            NaN                      NaN   \n",
       "4                            NaN                      NaN   \n",
       "...                          ...                      ...   \n",
       "4999995                      NaN                      NaN   \n",
       "4999996                      NaN                      NaN   \n",
       "4999997                      NaN                      NaN   \n",
       "4999998                      NaN                      NaN   \n",
       "4999999                      NaN                      NaN   \n",
       "\n",
       "        secondary_notify_party_10 actual_arrival_date  \n",
       "0                             NaN          2017-10-10  \n",
       "1                             NaN          2018-01-30  \n",
       "2                             NaN          2018-03-30  \n",
       "3                             NaN          2018-03-30  \n",
       "4                             NaN          2018-03-30  \n",
       "...                           ...                 ...  \n",
       "4999995                       NaN          2019-04-05  \n",
       "4999996                       NaN          2019-04-05  \n",
       "4999997                       NaN          2019-04-05  \n",
       "4999998                       NaN          2019-04-05  \n",
       "4999999                       NaN          2019-04-05  \n",
       "\n",
       "[5000000 rows x 34 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Currated layer only requires : container |  header  |  tariff   |   billgen   \n",
    "dfs = gen_dfs(path='header')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will not be using\n",
    "drop_dfs_col(dfs, rm_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into one \n",
    "dfs = merge_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate records\n",
    "rm_dupe(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the weight unit to a singular unit ( Kilogram )\n",
    "universal_unit(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove out the Outliers\n",
    "dfs = IQR_outlier(dfs, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identifier : False\n",
      "port_of_unlading : False\n",
      "estimated_arrival_date : False\n",
      "foreign_port_of_lading : False\n",
      "weight : False\n",
      "weight_unit : False\n",
      "actual_arrival_date : False\n"
     ]
    }
   ],
   "source": [
    "# Check to make sure there are no null values 😀\n",
    "cols_with_null(dfs)\n",
    "# Header does not need to deal with any Nulls 🎉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16564623    1959-11-10\n",
       "16393523    1959-11-10\n",
       "18272573    2000-12-07\n",
       "18276281    2000-12-07\n",
       "18276282    2000-12-07\n",
       "               ...    \n",
       "17626392    2020-05-01\n",
       "13845421    2021-12-29\n",
       "13851311    2021-12-29\n",
       "13851304    2021-12-29\n",
       "13845425    2021-12-29\n",
       "Name: estimated_arrival_date, Length: 17619371, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['estimated_arrival_date'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>port_of_unlading</th>\n",
       "      <th>estimated_arrival_date</th>\n",
       "      <th>foreign_port_of_lading</th>\n",
       "      <th>weight</th>\n",
       "      <th>weight_unit</th>\n",
       "      <th>actual_arrival_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201901010</td>\n",
       "      <td>Oakland, California</td>\n",
       "      <td>2017-10-08</td>\n",
       "      <td>Tanjung Priok,Indonesia</td>\n",
       "      <td>9904</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>2017-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201901011</td>\n",
       "      <td>Los Angeles, California</td>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>Singapore,Singapore</td>\n",
       "      <td>857</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>2018-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201901012</td>\n",
       "      <td>Norfolk, Virginia</td>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>Rotterdam,Netherlands</td>\n",
       "      <td>44599</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>2018-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201901013</td>\n",
       "      <td>Norfolk, Virginia</td>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>Bremerhaven,Federal Republic of Germany</td>\n",
       "      <td>9258</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>2018-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201901014</td>\n",
       "      <td>Norfolk, Virginia</td>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>Bremerhaven,Federal Republic of Germany</td>\n",
       "      <td>34760</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>2018-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19336851</th>\n",
       "      <td>2019123164866</td>\n",
       "      <td>Oakland, California</td>\n",
       "      <td>2019-08-09</td>\n",
       "      <td>Shanghai ,China (Mainland)</td>\n",
       "      <td>19950</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>2019-08-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19336852</th>\n",
       "      <td>2019123164867</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>2019-12-10</td>\n",
       "      <td>Tampico,Mexico</td>\n",
       "      <td>1794</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>2019-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19336853</th>\n",
       "      <td>2019123164868</td>\n",
       "      <td>Tacoma, Washington</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>Vung Tau,Vietnam</td>\n",
       "      <td>7517</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>2019-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19336854</th>\n",
       "      <td>2019123164869</td>\n",
       "      <td>Los Angeles, California</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>Xiamen,China (Mainland)</td>\n",
       "      <td>19260</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>2019-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19336855</th>\n",
       "      <td>2019123164870</td>\n",
       "      <td>Tacoma, Washington</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>Vung Tau,Vietnam</td>\n",
       "      <td>6556</td>\n",
       "      <td>Kilograms</td>\n",
       "      <td>2019-12-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17619371 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             identifier         port_of_unlading estimated_arrival_date  \\\n",
       "0             201901010      Oakland, California             2017-10-08   \n",
       "1             201901011  Los Angeles, California             2018-01-28   \n",
       "2             201901012        Norfolk, Virginia             2018-03-28   \n",
       "3             201901013        Norfolk, Virginia             2018-03-28   \n",
       "4             201901014        Norfolk, Virginia             2018-03-28   \n",
       "...                 ...                      ...                    ...   \n",
       "19336851  2019123164866      Oakland, California             2019-08-09   \n",
       "19336852  2019123164867           Houston, Texas             2019-12-10   \n",
       "19336853  2019123164868       Tacoma, Washington             2019-12-06   \n",
       "19336854  2019123164869  Los Angeles, California             2019-11-30   \n",
       "19336855  2019123164870       Tacoma, Washington             2019-12-06   \n",
       "\n",
       "                           foreign_port_of_lading  weight weight_unit  \\\n",
       "0                         Tanjung Priok,Indonesia    9904   Kilograms   \n",
       "1                             Singapore,Singapore     857   Kilograms   \n",
       "2                           Rotterdam,Netherlands   44599   Kilograms   \n",
       "3         Bremerhaven,Federal Republic of Germany    9258   Kilograms   \n",
       "4         Bremerhaven,Federal Republic of Germany   34760   Kilograms   \n",
       "...                                           ...     ...         ...   \n",
       "19336851               Shanghai ,China (Mainland)   19950   Kilograms   \n",
       "19336852                           Tampico,Mexico    1794   Kilograms   \n",
       "19336853                         Vung Tau,Vietnam    7517   Kilograms   \n",
       "19336854                  Xiamen,China (Mainland)   19260   Kilograms   \n",
       "19336855                         Vung Tau,Vietnam    6556   Kilograms   \n",
       "\n",
       "         actual_arrival_date  \n",
       "0                 2017-10-10  \n",
       "1                 2018-01-30  \n",
       "2                 2018-03-30  \n",
       "3                 2018-03-30  \n",
       "4                 2018-03-30  \n",
       "...                      ...  \n",
       "19336851          2019-08-09  \n",
       "19336852          2019-11-16  \n",
       "19336853          2019-12-14  \n",
       "19336854          2019-12-01  \n",
       "19336855          2019-12-14  \n",
       "\n",
       "[17619371 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the type of column\n",
    "rm_inval_type(dfs, 'weight', int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned up Header Data -> Store into \"Silver Layer\" \n",
    "df2csv(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Export Modified Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Establish a connection to the MySQL database\n",
    "\n",
    "Returns: \n",
    "\"\"\"\n",
    "def link_db():\n",
    "    try:\n",
    "        return pymssql.connect(server='DESKTOP-N9SA336')\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = link_db()\n",
    "if conn:\n",
    "    cursor = conn.cursor() # Cursors are database level objects that let your query a database multiple times - Think of it as a pointer to a row\n",
    "    cursor.execute(\"SELECT * FROM INFORMATION_SCHEMA.TABLES\")\n",
    "    rows = cursor.fetchall()\n",
    "    [print(row) for row in rows]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
